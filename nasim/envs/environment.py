""" The main Environment class for NASim: NASimEnv.

The NASimEnv class is the main interface for agents interacting with NASim.
"""
import gym
from gym import spaces
import numpy as np

from nasim.envs.state import State
from nasim.envs.render import Viewer
from nasim.envs.attacker.network import Network as A_n
from nasim.envs.defender.network import Network as D_n
from nasim.envs.attacker.observation import Observation as A_o
from nasim.envs.defender.observation import Observation as D_o
# *** added *** Action space classes
from nasim.envs.attacker.action import Action, FlatActionSpace
from nasim.envs.defender.action import Action_Defender, FlatDefenderActionSpace

# Test
from nasim.envs.attacker.observation import Observation
from nasim.envs.defender.observation import Observation as DObservation

class NASimEnv(gym.Env):
    """ A simulated computer network environment for pen-testing.

    Implements the gymnasium interface.

    ...

    Attributes
    ----------
    name : str
        the environment scenario name
    scenario : Scenario
        Scenario object, defining the properties of the environment
    action_space : FlatActionSpace or ParameterisedActionSpace
        Action space for environment.
        If *flat_action=True* then this is a discrete action space (which
        subclasses gymnasium.spaces.Discrete), so each action is represented by an
        integer.
        If *flat_action=False* then this is a parameterised action space (which
        subclasses gymnasium.spaces.MultiDiscrete), so each action is represented
        using a list of parameters.
    observation_space : gymnasium.spaces.Box
        observation space for environment.
        If *flat_obs=True* then observations are represented by a 1D vector,
        otherwise observations are represented as a 2D matrix.
    current_state : State
        the current state of the environment
    last_obs : Observation
        the last observation that was generated by environment
    steps : int
        the number of steps performed since last reset (this does not include
        generative steps)

    """
    metadata = {'render_modes': ["human", "ansi"]}
    render_mode = None
    reward_range = (-float('inf'), float('inf'))

    action_space = None
    observation_space = None
    current_state = None
    last_obs = None

    defender_action_space = None
    D_current_state = None
    D_last_obs = None

    def __init__(self,
                 scenario,
                 fully_obs=False,
                 flat_actions=True,
                 flat_obs=True,
                 render_mode=None
                 ):
        """
        Parameters
        ----------
        scenario : Scenario
            Scenario object, defining the properties of the environment
        fully_obs : bool, optional
            The observability mode of environment, if True then uses fully
            observable mode, otherwise is partially observable (default=False)
        flat_actions : bool, optional
            If true then uses a flat action space, otherwise will uses a
            parameterised action space (default=True).
        flat_obs : bool, optional
            If true then uses a 1D observation space, otherwise uses a 2D
            observation space (default=True)
        render_mode : str, optional
            The render mode to use for the environment.
        """
        self.name = scenario.name
        self.scenario = scenario
        self.fully_obs = fully_obs
        self.flat_actions = flat_actions
        self.flat_obs = flat_obs
        self.render_mode = render_mode

        self.A_network = A_n(scenario)
        self.D_network = D_n(scenario)

        self.current_state = State.generate_initial_state(self.A_network)
        self.D_current_state = State.D_generate_initial_state(self.D_network)

        self._renderer = None
        self.reset()

        if self.flat_actions:
            self.action_space = FlatActionSpace(self.scenario)
            self.defender_action_space = FlatDefenderActionSpace(self.scenario)
        else:
            print('error : Non parameterised Action')

        if self.flat_obs:
            obs_shape = self.last_obs.shape_flat()
            D_obs_shape = self.D_last_obs.shape_flat()
        else:
            obs_shape = self.last_obs.shape()
            D_obs_shape = self.D_last_obs.shape()

        obs_low, obs_high = A_o.get_space_bounds(self.scenario)
        self.A_observation_space = spaces.Box(
            low=obs_low, high=obs_high, shape=obs_shape
        )

        D_obs_low, D_obs_high = D_o.get_space_bounds(self.scenario)
        self.D_observation_space = spaces.Box(
            low=D_obs_low, high=D_obs_high, shape=D_obs_shape
        )

        self.steps = 0
        self.D_steps = 0

    def reset(self, *, seed=None, options=None):
        super().reset(seed=seed, options=options)
        self.steps = 0
        self.D_steps = 0

        self.current_state = self.A_network.reset_with_pps(self.current_state)
        self.D_current_state = self.D_network.reset_with_pps(self.D_current_state)

        self.last_obs = self.current_state.get_initial_observation(
            self.fully_obs
        )

        self.D_last_obs = self.D_current_state.get_D_initial_observation(
            self.fully_obs
        )

        if self.flat_obs:
            obs = self.last_obs.numpy_flat()
            d_obs = self.D_last_obs.numpy_flat()
        else:
            obs = self.last_obs.numpy()
            d_obs = self.D_last_obs.numpy()

        return obs, d_obs, {}

    def get_train_obs(self):
        A_obs = self.current_state.get_D_initial_observation(
            self.fully_obs
        )
        D_obs = self.D_current_state.get_D_initial_observation(
            self.fully_obs
        )

        return A_obs.numpy_flat(), D_obs.numpy_flat()


    def step(self, action, agent:str, mode:str):
        if agent == "attacker":
            if mode == 'multi':
                State.sync_dicts_by_value(self.D_current_state, self.current_state, agent)

            next_state, obs, reward, done, action, info = self.generative_step(
                self.current_state,
                action
            )
            self.current_state = next_state
            self.last_obs = obs

            if self.flat_obs:
                obs = obs.numpy_flat()

            else:
                obs = obs.numpy()
            self.steps += 1

            step_limit_reached = (
                    self.scenario.step_limit is not None
                    and self.steps >= self.scenario.step_limit
            )
            return obs, reward, done, step_limit_reached, info

        elif agent == "defender":
            if mode == 'multi':
                State.sync_dicts_by_value(self.current_state, self.D_current_state, agent)

            next_state, obs, reward, done, action, info = self.generative_defender_step(
                self.D_current_state,
                action
            )
            self.D_current_state = next_state
            self.D_last_obs = obs

            if self.flat_obs:
                obs = obs.numpy_flat()
            else:
                obs = obs.numpy()

            self.D_steps += 1

            step_limit_reached = (
                    self.scenario.step_limit is not None
                    and self.D_steps >= self.scenario.step_limit
            )
            return obs, reward, done, step_limit_reached, info

    def generative_step(self, state, action):
        if not isinstance(action, Action):
            action = self.action_space.get_action(action)

        next_state, action_obs = self.A_network.perform_action_with_pps_v2(
            state, action
        )
        obs = next_state.get_observation(
            action, action_obs, self.fully_obs
        )
        done = self.attacker_goal_reached(next_state)

        reward = action_obs.value - action.cost
        return next_state, obs, reward, done, action, action_obs.info()

    def generative_defender_step(self, state, action):
        if not isinstance(action, Action_Defender):
            action = self.defender_action_space.get_action(action)

        next_state, action_obs = self.D_network.perform_defender_action(
            state, action
        )
        obs = next_state.get_observation_defender(
            action, action_obs, self.fully_obs
        )
        done = False
        reward = action_obs.value - action.cost

        return next_state, obs, reward, done, action, action_obs.info()

    def generate_initial_state(self):
        return State.generate_initial_state(self.A_network)

    def D_generate_initial_state(self):
        return State.D_generate_initial_state(self.D_network)

    def render(self):
        if self.render_mode is None:
            return
        return self.render_obs(mode=self.render_mode, obs=self.last_obs)

    def render_obs(self, mode="human", obs=None):
        if mode is None:
            return

        if obs is None:
            obs = self.last_obs

        if not isinstance(obs, A_o):
            obs = A_o.from_numpy(obs, self.current_state.shape())

        if self._renderer is None:
            self._renderer = Viewer(self.A_network)

        if mode in ("human", "ansi"):
            return self._renderer.render_readable(obs)
        else:
            raise NotImplementedError(
                "Please choose correct render mode from :"
                f"{self.metadata['render_modes']}"
            )

    def D_render_obs(self, mode="human", obs=None):
        if mode is None:
            return

        if obs is None:
            obs = self.D_last_obs

        if not isinstance(obs, D_o):
            obs = D_o.from_numpy(obs, self.D_current_state.shape())

        if self._renderer is None:
            self._renderer = Viewer(self.D_network)

        if mode in ("human", "ansi"):
            return self._renderer.render_readable(obs)
        else:
            raise NotImplementedError(
                "Please choose correct render mode from :"
                f"{self.metadata['render_modes']}"
            )

    def render_state(self, mode="human", state=None):
        if mode is None:
            return

        if state is None:
            state = self.current_state

        if not isinstance(state, State):
            state = State.from_numpy(state,
                                     self.current_state.shape(),
                                     self.current_state.host_num_map)

        if self._renderer is None:
            self._renderer = Viewer(self.A_network)

        if mode in ("human", "ansi"):
            return self._renderer.render_readable_state(state)
        else:
            raise NotImplementedError(
                "Please choose correct render mode from : "
                f"{self.metadata['render_modes']}"
            )

    def render_d_state(self, mode="human", state=None):
        if mode is None:
            return

        if state is None:
            state = self.D_current_state

        if not isinstance(state, State):
            state = State.from_numpy(state,
                                     self.D_current_state.shape(),
                                     self.D_current_state.host_num_map)

        if self._renderer is None:
            self._renderer = Viewer(self.D_network)

        if mode in ("human", "ansi"):
            return self._renderer.render_d_readable_state(state)
        else:
            raise NotImplementedError(
                "Please choose correct render mode from : "
                f"{self.metadata['render_modes']}"
            )

    def render_action(self, action):
        if not isinstance(action, Action):

            action = self.action_space.get_action(action)
        print(action)

    def render_episode(self, episode, width=7, height=7):
        if self._renderer is None:
            self._renderer = Viewer(self.A_network)
        self._renderer.render_episode(episode, width, height)

    def render_network_graph(self, ax=None, show=False):
        if self._renderer is None:
            self._renderer = Viewer(self.A_network)
        state = self.current_state
        self._renderer.render_graph(state, ax, show)

    def get_minimal_steps(self):
        return self.A_network.get_minimal_steps()

    def get_action_mask(self):
        assert isinstance(self.action_space, FlatActionSpace), \
            "Can only use action mask function when using flat action space"
        mask = np.zeros(self.action_space.n, dtype=np.int64)
        for a_idx in range(self.action_space.n):
            action = self.action_space.get_action(a_idx)
            if self.A_network.host_discovered(action.target):
                mask[a_idx] = 1
        return mask

    def get_score_upper_bound(self):
        max_reward = self.A_network.get_total_sensitive_host_value()
        max_reward += self.A_network.get_total_discovery_value()
        max_reward -= self.A_network.get_minimal_steps()
        return max_reward

    def attacker_goal_reached(self, state=None):
        if state is None:
            state = self.current_state
        return self.A_network.all_sensitive_hosts_compromised(state)

    def defender_goal_reached(self, state=None):
        if state is None:
            state = self.D_current_state
        return self.D_network.all_sensitive_hosts_compromised(state)


    def __str__(self):
        output = [
            "NASimEnv:",
            f"name={self.name}",
            f"fully_obs={self.fully_obs}",
            f"flat_actions={self.flat_actions}",
            f"flat_obs={self.flat_obs}"
        ]
        return "\n  ".join(output)

    def close(self):
        if self._renderer is not None:
            self._renderer.close()
            self._renderer = None